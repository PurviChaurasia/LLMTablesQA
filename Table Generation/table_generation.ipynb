{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPORTSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r\"D:\\NSFQA\\Table Generation\\sportsett_train.csv\")\n",
    "df2 = pd.read_csv(r\"D:\\NSFQA\\Table Generation\\sportsett_test.csv\")\n",
    "df3 = pd.read_csv(r\"D:\\NSFQA\\Table Generation\\sportsett_dev.csv\")\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split By Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_years = df['year'].unique()\n",
    "for year in unique_years:\n",
    "    df_year = df[df['year'] == year]\n",
    "    if len(df_year) >= 60:\n",
    "        sample = df_year.sample(30)\n",
    "        file_name = f\"sportset_year_{year}_60_34.csv\"\n",
    "        sample.to_csv(file_name, index=False)\n",
    "\n",
    "        print(f\"CSV file created for year {year}: {file_name}\")\n",
    "        df = df.drop(sample.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_states = df['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pennsylvania', 'Wisconsin', 'England', 'Illinois', 'Ohio',\n",
       "       'Massachusetts', 'California', 'Tennessee', 'Georgia', 'Florida',\n",
       "       'North Carolina', 'Utah', 'New York', 'Texas', 'Colorado',\n",
       "       'Indiana', 'Louisiana', 'Michigan', 'Ontario', 'Arizona',\n",
       "       'Oklahoma', 'Minnesota', 'Oregon', 'Washington', 'Mexico'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "California        1026\n",
       "Texas              411\n",
       "New York           409\n",
       "Florida            406\n",
       "Tennessee          205\n",
       "Georgia            205\n",
       "Ohio               205\n",
       "Massachusetts      205\n",
       "Illinois           205\n",
       "Oklahoma           205\n",
       "Minnesota          205\n",
       "Ontario            205\n",
       "North Carolina     205\n",
       "Louisiana          205\n",
       "Indiana            205\n",
       "Michigan           205\n",
       "Pennsylvania       204\n",
       "Washington         204\n",
       "Wisconsin          204\n",
       "Colorado           204\n",
       "Utah               204\n",
       "Oregon             204\n",
       "Arizona            202\n",
       "Mexico               7\n",
       "England              5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regions\n",
    "northeast = ['Pennsylvania', 'Massachusetts', 'New York']\n",
    "midwest = ['Wisconsin', 'Illinois', 'Ohio', 'Indiana', 'Michigan', 'Minnesota']\n",
    "south = ['Tennessee', 'Georgia', 'Florida', 'North Carolina', 'Texas', 'Louisiana', 'Oklahoma']\n",
    "west = ['California', 'Utah', 'Colorado', 'Arizona', 'Oregon', 'Washington']\n",
    "\n",
    "# Create sub-tables (dataframes) for each region\n",
    "northeast_df = df[df['state'].isin(northeast)]\n",
    "midwest_df = df[df['state'].isin(midwest)]\n",
    "south_df = df[df['state'].isin(south)]\n",
    "west_df = df[df['state'].isin(west)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(west_df) >= 60:\n",
    "    sample = west_df.sample(60)\n",
    "    file_name = f\"sportset_west_60_40.csv\"\n",
    "    sample.to_csv(file_name, index=False)\n",
    "    west_df = west_df.drop(sample.index)\n",
    "    print(west_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by Climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regions\n",
    "cold_temperate = ['Pennsylvania', 'Wisconsin', 'Illinois', 'Ohio', 'Massachusetts', 'New York', \n",
    "                  'Michigan', 'Minnesota', 'Oregon', 'Washington', 'Ontario']\n",
    "warm_hot = ['Georgia', 'Florida', 'Texas', 'Arizona', 'California', 'Tennessee', 'Louisiana', \n",
    "            'Mexico', 'Oklahoma', 'Utah', 'Colorado']\n",
    "\n",
    "# Create sub-tables (dataframes) for each region\n",
    "cold_temperate_df = df[df['state'].isin(cold_temperate)]\n",
    "warm_hot_df = df[df['state'].isin(warm_hot)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(warm_hot_df) >= 60:\n",
    "    sample = warm_hot_df.sample(60)\n",
    "    file_name = f\"sportset_warm_hot_60_10.csv\"\n",
    "    sample.to_csv(file_name, index=False)\n",
    "    warm_hot_df = warm_hot_df.drop(sample.index)\n",
    "    print(warm_hot_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSF\n",
    "1. For Year wise tables are already present in yearwise format, just divide into smaller sub tables\n",
    "2. Apart from that have to think of other markers for dividing tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = r'D:\\LLMTables\\nsf_data\\csv_output'  # Folder containing the CSV files\n",
    "sub_table_folder = r'D:\\LLMTables\\Tables\\NSF\\Year-wise'  # Folder to store the sub-tables\n",
    "\n",
    "if not os.path.exists(sub_table_folder):\n",
    "    os.makedirs(sub_table_folder)\n",
    "\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "for filename in csv_files:\n",
    "    n_rows = 60\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    print(filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(df.shape)\n",
    "    \n",
    "    if (df.shape[0]) < n_rows:\n",
    "        print(f\"{filename} does not have {n_rows}\")\n",
    "        continue\n",
    "    sub_table = df.sample(n_rows)\n",
    "    sub_table_filename = f\"sub_{filename}\"\n",
    "    sub_table_path = os.path.join(sub_table_folder, sub_table_filename)\n",
    "    \n",
    "    sub_table.to_csv(sub_table_path, index=False)\n",
    "    \n",
    "    print(f\"Extracted sub-table from {filename} and saved as {sub_table_filename}\")\n",
    "\n",
    "print(\"Sub-tables creation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 year periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    '1959_data.csv', '1960_data.csv', '1961_data.csv', '1962_data.csv',\n",
    "    '1963_data.csv', '1964_data.csv', '1965_data.csv', '1966_data.csv',\n",
    "    '1967_data.csv', '1968_data.csv', '1969_data.csv', '1970_data.csv',\n",
    "    '1971_data.csv', '1972_data.csv', '1973_data.csv', '1974_data.csv',\n",
    "    '1975_data.csv', '1976_data.csv', '1977_data.csv', '1978_data.csv',\n",
    "    '1979_data.csv', '1980_data.csv', '1981_data.csv', '1982_data.csv',\n",
    "    '1983_data.csv', '1984_data.csv', '1985_data.csv', '1986_data.csv',\n",
    "    '1987_data.csv', '1988_data.csv', '1989_data.csv', '1990_data.csv',\n",
    "    '1991_data.csv', '1992_data.csv', '1993_data.csv', '1994_data.csv',\n",
    "    '1995_data.csv', '1996_data.csv', '1997_data.csv', '1998_data.csv',\n",
    "    '1999_data.csv', '2000_data.csv', '2001_data.csv', '2002_data.csv',\n",
    "    '2003_data.csv', '2004_data.csv', '2005_data.csv', '2006_data.csv',\n",
    "    '2007_data.csv', '2008_data.csv', '2009_data.csv', '2010_data.csv',\n",
    "    '2011_data.csv', '2012_data.csv', '2013_data.csv', '2014_data.csv',\n",
    "    '2015_data.csv', '2016_data.csv', '2017_data.csv', '2018_data.csv',\n",
    "    '2019_data.csv', '2020_data.csv', '2021_data.csv', '2022_data.csv',\n",
    "    '2023_data.csv'\n",
    "]\n",
    "def divide_into_periods(filenames):\n",
    "    periods = {}\n",
    "    for filename in filenames:\n",
    "        year = int(filename.split('_')[0])  # Extract the year\n",
    "        period_start = year - (year % 5)  # Calculate the start of the 5-year period\n",
    "        period_key = f\"{period_start}-{period_start + 4}\"  # Define the period key\n",
    "        \n",
    "        if period_key not in periods:\n",
    "            periods[period_key] = []\n",
    "        \n",
    "        periods[period_key].append(filename)\n",
    "    \n",
    "    return periods\n",
    "\n",
    "periods = divide_into_periods(filenames)\n",
    "for period, files in periods.items():\n",
    "    print(f\"{period}: {files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "directory_path = r'D:\\LLMTables\\nsf_data\\csv_output'\n",
    "filenames = [\n",
    "    '1959_data.csv', '1960_data.csv', '1961_data.csv', '1962_data.csv',\n",
    "    '1963_data.csv', '1964_data.csv', '1965_data.csv', '1966_data.csv',\n",
    "    '1967_data.csv', '1968_data.csv', '1969_data.csv', '1970_data.csv',\n",
    "    '1971_data.csv', '1972_data.csv', '1973_data.csv', '1974_data.csv',\n",
    "    '1975_data.csv', '1976_data.csv', '1977_data.csv', '1978_data.csv',\n",
    "    '1979_data.csv', '1980_data.csv', '1981_data.csv', '1982_data.csv',\n",
    "    '1983_data.csv', '1984_data.csv', '1985_data.csv', '1986_data.csv',\n",
    "    '1987_data.csv', '1988_data.csv', '1989_data.csv', '1990_data.csv',\n",
    "    '1991_data.csv', '1992_data.csv', '1993_data.csv', '1994_data.csv',\n",
    "    '1995_data.csv', '1996_data.csv', '1997_data.csv', '1998_data.csv',\n",
    "    '1999_data.csv', '2000_data.csv', '2001_data.csv', '2002_data.csv',\n",
    "    '2003_data.csv', '2004_data.csv', '2005_data.csv', '2006_data.csv',\n",
    "    '2007_data.csv', '2008_data.csv', '2009_data.csv', '2010_data.csv',\n",
    "    '2011_data.csv', '2012_data.csv', '2013_data.csv', '2014_data.csv',\n",
    "    '2015_data.csv', '2016_data.csv', '2017_data.csv', '2018_data.csv',\n",
    "    '2019_data.csv', '2020_data.csv', '2021_data.csv', '2022_data.csv',\n",
    "    '2023_data.csv'\n",
    "]\n",
    "\n",
    "sampled_indices_file = os.path.join(directory_path, 'sampled_indices.json')\n",
    "\n",
    "def load_sampled_indices():\n",
    "    if os.path.exists(sampled_indices_file):\n",
    "        with open(sampled_indices_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_sampled_indices(sampled_indices):\n",
    "    with open(sampled_indices_file, 'w') as f:\n",
    "        json.dump(sampled_indices, f)\n",
    "\n",
    "def divide_into_periods(filenames):\n",
    "    periods = {}\n",
    "    for filename in filenames:\n",
    "        year = int(filename.split('_')[0])\n",
    "        period_start = year - (year % 5)\n",
    "        period_key = f\"{period_start}-{period_start + 4}\"\n",
    "        \n",
    "        if period_key not in periods:\n",
    "            periods[period_key] = []\n",
    "        \n",
    "        periods[period_key].append(filename)\n",
    "    \n",
    "    return periods\n",
    "\n",
    "def process_periods(periods):\n",
    "    sampled_indices = load_sampled_indices()\n",
    "    \n",
    "    for period, files in periods.items():\n",
    "        data_frames = []\n",
    "        \n",
    "        for filename in files:\n",
    "            filepath = os.path.join(directory_path, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            data_frames.append(df)\n",
    "\n",
    "        combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "        available_indices = list(set(combined_df.index) - set(sampled_indices.get(period, [])))\n",
    "\n",
    "        if len(available_indices) < 30:\n",
    "            print(f\"Not enough unique rows available for {period}. Sampling {len(available_indices)} rows instead.\")\n",
    "            continue\n",
    "        else:\n",
    "            sampled_rows = pd.Series(available_indices).sample(n=30, random_state=1).tolist()\n",
    "\n",
    "        if period not in sampled_indices:\n",
    "            sampled_indices[period] = []\n",
    "        sampled_indices[period].extend(sampled_rows)\n",
    "\n",
    "        sampled_df = combined_df.loc[sampled_rows]\n",
    "\n",
    "        sampled_df.to_csv(os.path.join(f\"sampled_{period}_3.csv\"), index=False)\n",
    "        print(f\"Saved sampled data for {period} as 'sampled_{period}.csv'\")\n",
    "\n",
    "    save_sampled_indices(sampled_indices)\n",
    "\n",
    "periods = divide_into_periods(filenames)\n",
    "\n",
    "process_periods(periods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(df[\"Institution_CountryName\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid or malformed entries (like lists)\n",
    "df = df[~df['Institution_CountryName'].isin(['[]', '[None, None, None, None, None, None, None]', \"['United States', 'United States']\", \"['United States', 'United States', 'United States']\", \"['United States', 'United States', 'United States', 'United States']\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "continent_map = {\n",
    "    'United States': 'North America', 'Canada': 'North America', 'Bermuda': 'North America', 'Mexico': 'North America', 'Puerto Rico': 'North America', 'American Samoa': 'North America', 'Panama': 'North America', 'Micronesia': 'North America',\n",
    "    'United Kingdom': 'Europe', 'France': 'Europe', 'Slovenia': 'Europe', 'Germany': 'Europe', 'Poland': 'Europe', 'Croatia': 'Europe', 'Hungary': 'Europe', 'Sweden': 'Europe', 'Switzerland': 'Europe', 'Austria': 'Europe', 'Denmark': 'Europe', 'Italy': 'Europe', 'Czechoslovakia': 'Europe', 'Netherlands': 'Europe', 'Bosnia and Herzegovina': 'Europe', 'Finland': 'Europe', 'Estonia': 'Europe', 'Spain': 'Europe', 'Belgium': 'Europe', 'Portugal': 'Europe', 'Bulgaria': 'Europe', 'Czech Republic': 'Europe', 'Yugoslavia': 'Europe',\n",
    "    'India': 'Asia', 'Pakistan': 'Asia', 'Japan': 'Asia', 'Israel': 'Asia', 'Turkey': 'Asia', 'Iran': 'Asia', 'Indonesia': 'Asia', 'Taiwan': 'Asia', 'China': 'Asia', 'Sri Lanka': 'Asia',\n",
    "    'Egypt': 'Africa', 'Tunisia': 'Africa', 'Cameroon': 'Africa', 'South Africa': 'Africa', 'Kenya': 'Africa',\n",
    "    'Brazil': 'South America', 'Argentina': 'South America', 'Chile': 'South America', 'Venezuela': 'South America', 'Colombia': 'South America', 'Peru': 'South America',\n",
    "    'Australia': 'Australia', 'New Zealand': 'Australia', 'Australasia': 'Australia'\n",
    "}\n",
    "\n",
    "df['continent'] = df['Institution_CountryName'].map(continent_map)\n",
    "\n",
    "df['continent'].fillna('Unknown', inplace=True)\n",
    "\n",
    "asia = df[df['continent'] == 'Asia']\n",
    "africa = df[df['continent'] == 'Africa']\n",
    "north_america = df[df['continent'] == 'North America']\n",
    "south_america = df[df['continent'] == 'South America']\n",
    "europe = df[df['continent'] == 'Europe']\n",
    "australia = df[df['continent'] == 'Australia']\n",
    "\n",
    "print(\"Asia Sub-table:\\n\", asia)\n",
    "print(\"Africa Sub-table:\\n\", africa)\n",
    "print(\"North America Sub-table:\\n\", north_america)\n",
    "print(\"South America Sub-table:\\n\", south_america)\n",
    "print(\"Europe Sub-table:\\n\", europe)\n",
    "print(\"Australia Sub-table:\\n\", australia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(australia) >= 30:\n",
    "    sample = australia.sample(30)\n",
    "    file_name = f\"nsf_continent_australia_1.csv\"\n",
    "    sample.to_csv(file_name, index=False)\n",
    "    australia = australia.drop(sample.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(df[\"Institution_StateName\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table_sizes = [(20, 30), (15, 40), (15, 60)]  # (number of tables, rows per table)\n",
    "org_row_count = dict(df[\"Organization_Directorate_LongName\"].value_counts())\n",
    "\n",
    "for org in org_row_count.keys():\n",
    "\n",
    "    org_df = df[df['Organization_Directorate_LongName'] == org]\n",
    "    table_count = 1\n",
    "    for num_tables, rows_per_table in table_sizes:\n",
    "        \n",
    "        for _ in range(num_tables):\n",
    "            if len(org_df) >= rows_per_table:\n",
    "                sample = org_df.sample(rows_per_table)\n",
    "                file_name = f\"nsf_{org}_{rows_per_table}_{table_count}.csv\"\n",
    "                sample.to_csv(file_name, index=False)\n",
    "                # Remove sampled rows from the original DataFrame\n",
    "                org_df = org_df.drop(sample.index)\n",
    "\n",
    "                print(f\"table number {table_count} created for {org} with rows {rows_per_table}\")\n",
    "                table_count += 1\n",
    "                \n",
    "            else:\n",
    "                print(f\"Not enough rows for {org} to create table {table_count} with {rows_per_table} rows.\")\n",
    "                break \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Org Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(df[\"Organization_Directorate_LongName\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(df[\"Organization_Division_LongName\"].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "purvi-nsfqa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
