{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a teacher\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of france?\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Database from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sportset_2\n",
      "Custom schema applied for table 'sportset_2'.\n",
      "sportset_coldtemp_30_13\n",
      "Custom schema applied for table 'sportset_coldtemp_30_13'.\n",
      "sportset_midwest_30_8\n",
      "Custom schema applied for table 'sportset_midwest_30_8'.\n",
      "sportset_northeast_30_1\n",
      "Custom schema applied for table 'sportset_northeast_30_1'.\n",
      "sportset_west_30_4\n",
      "Custom schema applied for table 'sportset_west_30_4'.\n",
      "All CSV files have been successfully imported with the custom schema.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def csv_folder_to_database_custom_schema(folder_path, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            table_name = os.path.splitext(filename)[0].replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "            print(table_name)\n",
    "            cursor.execute(f\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS {table_name}(\n",
    "                        day INTEGER NOT NULL CHECK (day BETWEEN 1 AND 31),\n",
    "                        month TEXT NOT NULL,\n",
    "                        year INTEGER NOT NULL CHECK (year BETWEEN 1800 AND 2100),\n",
    "                        dayname TEXT NOT NULL,\n",
    "                        season INTEGER NOT NULL,\n",
    "                        stadium TEXT NOT NULL,\n",
    "                        city TEXT NOT NULL,\n",
    "                        state TEXT NOT NULL,\n",
    "                        attendance INTEGER NOT NULL,\n",
    "                        capacity INTEGER NOT NULL,\n",
    "                        game_id INTEGER PRIMARY KEY,\n",
    "                        summary TEXT\n",
    "                    )\n",
    "                \"\"\")\n",
    "            print(f\"Custom schema applied for table '{table_name}'.\")\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                placeholders = ', '.join(['?'] * len(row))\n",
    "                column_names = ', '.join(row.index)\n",
    "                insert_query = f\"INSERT INTO {table_name} ({column_names}) VALUES ({placeholders})\"\n",
    "                cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "    conn.commit()\n",
    "    print(\"All CSV files have been successfully imported with the custom schema.\")\n",
    "    return conn\n",
    "\n",
    "folder_path = r'D:\\NSFQA\\Question Generation\\TestTables_5'\n",
    "db_path = r'D:\\NSFQA\\Question Generation\\SQL\\new_database.db'\n",
    "\n",
    "conn = csv_folder_to_database_custom_schema(folder_path, db_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: ['sportset_2', 'sportset_coldtemp_30_13', 'sportset_midwest_30_8', 'sportset_northeast_30_1', 'sportset_west_30_4']\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "db_path = r\"D:\\NSFQA\\Question Generation\\SQL\\new_database.db\" \n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = [row[0] for row in cursor.fetchall()]\n",
    "print(\"Tables in the database:\", tables)\n",
    "\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    dayname\\n0  Saturday\\n1  Saturday\\n2   Tuesday'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "result = pd.read_sql_query(\"SELECT dayname FROM sportset_2 WHERE attendance > 17000 AND state = 'Tennessee'\", conn)\n",
    "str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: {'day': 'INTEGER', 'month': 'TEXT', 'year': 'INTEGER', 'dayname': 'TEXT', 'season': 'INTEGER', 'stadium': 'TEXT', 'city': 'TEXT', 'state': 'TEXT', 'attendance': 'INTEGER', 'capacity': 'INTEGER', 'game_id': 'INTEGER', 'summary': 'TEXT'}\n",
      "\n",
      "Sample Rows:\n",
      "   day     month  year    dayname  season                  stadium  \\\n",
      "0   24   January  2015   Saturday    2014               FedExForum   \n",
      "1    3  December  2014  Wednesday    2014  Time Warner Cable Arena   \n",
      "2   13  December  2014   Saturday    2014  Time Warner Cable Arena   \n",
      "3   20     March  2015     Friday    2014             Amway Center   \n",
      "4   28   October  2014    Tuesday    2014     Smoothie King Center   \n",
      "\n",
      "          city           state  attendance  capacity  game_id  \\\n",
      "0      Memphis       Tennessee       17600     17800      269   \n",
      "1    Charlotte  North Carolina       16900     19100      379   \n",
      "2    Charlotte  North Carolina       17100     19100      382   \n",
      "3      Orlando         Florida       16200     18800      608   \n",
      "4  New Orleans       Louisiana       17100     16900      780   \n",
      "\n",
      "                                             summary  \n",
      "0  The Memphis Grizzlies ( 31 - 12 ) beat the Phi...  \n",
      "1  The Bulls ( 12 - 7 ) knocked off the Hornets (...  \n",
      "2  The Brooklyn Nets ( 10 - 12 ) defeated the Cha...  \n",
      "3  The Orlando Magic ( 22 - 49 ) defeated the Por...  \n",
      "4  The New Orleans Pelicans ( 10 ) defeated the O...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def get_table_schema_and_rows(table_name):\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    schema = cursor.fetchall()\n",
    "    schema_dict = {col[1]: col[2] for col in schema} \n",
    "    query = f\"SELECT * FROM {table_name} LIMIT 5\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    return schema_dict, df\n",
    "\n",
    "table_name = tables[0]\n",
    "schema, rows = get_table_schema_and_rows(table_name)\n",
    "print(\"Schema:\", schema)\n",
    "print(\"\\nSample Rows:\")\n",
    "print(rows)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'generate_queries_prompt' from 'prompts' (d:\\NSFQA\\Question Generation\\SQL\\prompts.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlite3\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_queries_prompt\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize OpenAI API (replace with your credentials)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m db_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNSFQA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mQuestion Generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSQL\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnew_database.db\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Update with the correct path to your database\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'generate_queries_prompt' from 'prompts' (d:\\NSFQA\\Question Generation\\SQL\\prompts.py)"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import random\n",
    "import json\n",
    "import sqlite3\n",
    "db_path = r\"D:\\LLMTables\\Question Generation\\test\\new_database_2.db\"  \n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_queries(table_name, schema, rows):\n",
    "    row_samples = [rows.sample(1, random_state=random.randint(1, 1000)).to_dict(orient=\"records\")[0] for _ in range(5)]\n",
    "    print(f\"Generating queries for {table_name}\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Act as an expert in SQL and databases. Please give valid output JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "         Read the table schema and 5 rows given from the table carefully and understand it correctly - \n",
    "         \n",
    "         Table Schema:\n",
    "         {json.dumps(schema, indent=4)}\n",
    "\n",
    "         ROW DATA:\n",
    "         {json.dumps(row_samples, indent=4)}\n",
    "\n",
    "         SQL TEMPLATE:\n",
    "\n",
    "         SELECT [column] FROM {table_name} WHERE [condition1] AND [condition2]\n",
    "\n",
    "         Instruction:\n",
    "            Please use the information from the table and data provided to fill in the placeholders in the template. Each SQL query should only return a single result using either:\n",
    "\n",
    "            - A specific column (e.g., city, attendance, capacity, etc.)\n",
    "            - An aggregate function (e.g., COUNT(), SUM(), MAX(), etc.)\n",
    "            Follow the template given and try to fill in the placeholders in a way that can lead to logical and complex queries.\n",
    "            Ensure that the queries generate deterministic answers, such as a single count, maximum, or specific column value (e.g., \"How many games were held in Orlando?\").\n",
    "            Generate 5 such SQL queries.\n",
    "            \n",
    "            Please follow the Response format while answering :\n",
    "            Query: <Single Liner SQL Query>\n",
    "            Table: {table_name}\n",
    "         \"\"\"}\n",
    "    ]\n",
    "\n",
    "    chat_completion, *_ = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages=messages,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "\n",
    "    ).choices\n",
    "    content = chat_completion.message.content\n",
    "    reply = json.loads(content)\n",
    "    return reply\n",
    "\n",
    "queries_json = generate_queries(table_name, schema, rows)\n",
    "\n",
    "print(\"Generated JSON Output:\")\n",
    "print(json.dumps(queries_json, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Query Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_tables_and_save_simple(db_path, output_file):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    final_result = []\n",
    "\n",
    "    for table_name in tables:\n",
    "        print(f\"Processing table: {table_name}\")\n",
    "\n",
    "        schema, rows = get_table_schema_and_rows(table_name)\n",
    "\n",
    "        queries_json = generate_queries(table_name, schema, rows)\n",
    "\n",
    "        for query in queries_json[\"queries\"]:\n",
    "            final_result.append({\n",
    "                \"table_name\": query[\"Table\"],\n",
    "                \"query\": query[\"Query\"] \n",
    "            })\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(final_result, f, indent=4)\n",
    "    \n",
    "    print(f\"JSON output saved to {output_file}\")\n",
    "\n",
    "db_path = r\"D:\\LLMTables\\Question Generation\\test\\new_database_2.db\"  \n",
    "output_file = r\"D:\\LLMTables\\Question Generation\\test\\simple_queries_output.json\"\n",
    "all_tables_result = process_all_tables_and_save_simple(db_path, output_file)\n",
    "\n",
    "print(\"Generated Simplified JSON Output for All Tables:\")\n",
    "print(json.dumps(all_tables_result, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Queries and Modify JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "\n",
    "def execute_queries_and_update_json(db_path, input_json_file, output_json_file):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    with open(input_json_file, \"r\") as f:\n",
    "        queries_json = json.load(f)\n",
    "\n",
    "    updated_queries = []\n",
    "\n",
    "    for entry in queries_json:\n",
    "        table_name = entry[\"table_name\"]\n",
    "        query = entry[\"query\"]\n",
    "        print(query)\n",
    "        \n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            result = cursor.fetchall()\n",
    "            print(result)\n",
    "            if len(result) == 1 and len(result[0]) == 1:\n",
    "                result_value = result[0][0]\n",
    "            else:\n",
    "                result_value = str(result)\n",
    "        except Exception as e:\n",
    "            result_value = f\"Error: {str(e)}\"\n",
    "\n",
    "        updated_queries.append({\n",
    "            \"table_name\": table_name,\n",
    "            \"query\": query,\n",
    "            \"result\": result_value\n",
    "        })\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    with open(output_json_file, \"w\") as f:\n",
    "        json.dump(updated_queries, f, indent=4)\n",
    "    \n",
    "    print(f\"Updated JSON with results saved to {output_json_file}\")\n",
    "    return updated_queries\n",
    "\n",
    "db_path = r\"D:\\LLMTables\\Question Generation\\test\\new_database_2.db\"\n",
    "input_json_file = r\"D:\\LLMTables\\Question Generation\\test\\simple_queries_output.json\"  \n",
    "output_json_file = r\"D:\\LLMTables\\Question Generation\\test\\queries_with_results.json\" \n",
    "\n",
    "updated_result = execute_queries_and_update_json(db_path, input_json_file, output_json_file)\n",
    "\n",
    "print(\"Updated JSON with Results:\")\n",
    "print(json.dumps(updated_result, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert SQL Query to Natural Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "def convert_sql_to_natural_language(input_json_file, output_json_file):\n",
    "    with open(input_json_file, \"r\") as f:\n",
    "        queries_json = json.load(f)\n",
    "    \n",
    "    updated_json = []\n",
    "    \n",
    "    for entry in queries_json:\n",
    "        sql_query = entry[\"query\"]\n",
    "        table_name = entry[\"table_name\"]\n",
    "        result = entry.get(\"result\", None)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        You are an expert data scientist skilled in SQL and natural language processing. Your task is to convert SQL queries into natural language questions. \n",
    "\n",
    "        Here is the SQL Query: \n",
    "        {sql_query}\n",
    "        The questions should:\n",
    "        - Clearly represent the intent of the SQL query.\n",
    "        - Translate technical terms, column headers, and values into natural, descriptive forms.\n",
    "        - Avoid technical jargon unless absolutely necessary.\n",
    "        - Ensure the question retains the same scope and meaning as the SQL query to avoid altering the query's answer.\n",
    "\n",
    "        For example:\n",
    "        SQL: SELECT COUNT(game_id) FROM sportset_2 WHERE city = 'Orlando' AND year = 2015\n",
    "        Output: How many games took place in Orlando in the year 2015?\n",
    "\n",
    "        Please convert this SQL query into a single natural language question. Ensure the column headers and values are human-readable. \n",
    "        Respond STRICTLY in the following format:\n",
    "        Question: <Natural Language Question>\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Act as an expert data scientist skilled in SQL and natural language processing.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:  \n",
    "            chat_completion, *_ = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=messages\n",
    "                ).choices\n",
    "            content = chat_completion.message.content\n",
    "            print(content)      \n",
    "            question = content.replace(\"Question: \", \"\").strip()\n",
    "        except Exception as e:\n",
    "            question = f\"Error generating question: {str(e)}\"\n",
    "        updated_json.append({\n",
    "            \"table_name\": table_name,\n",
    "            \"query\": sql_query,\n",
    "            \"result\": result,\n",
    "            \"question\": question\n",
    "        })\n",
    "    with open(output_json_file, \"w\") as f:\n",
    "        json.dump(updated_json, f, indent=4)\n",
    "    \n",
    "    print(f\"Updated JSON with natural language questions saved to {output_json_file}\")\n",
    "    return updated_json\n",
    "input_json_file = r\"D:\\LLMTables\\Question Generation\\test\\queries_with_results.json\"  \n",
    "output_json_file = r\"D:\\LLMTables\\Question Generation\\test\\natural_language_output.json\" \n",
    "\n",
    "updated_json = convert_sql_to_natural_language(input_json_file, output_json_file)\n",
    "\n",
    "print(\"Updated JSON with Natural Language Questions:\")\n",
    "print(json.dumps(updated_json, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "\n",
    "def convert_to_pipe_format(path_to_csv):\n",
    "    df = pd.read_csv(path_to_csv)\n",
    "    string = '/*\\n'\n",
    "    col_list = df.columns.values.tolist()\n",
    "    string += 'col : ' + ' | '.join(df.columns) + '\\n'\n",
    "    for row_id, row in df.iterrows():\n",
    "        string += f'row {row_id} : '\n",
    "        for column_id, header in enumerate(df.columns):\n",
    "            string += str(row[header])\n",
    "            if column_id != len(df.columns) - 1:\n",
    "                string += ' | '\n",
    "        string += '\\n'\n",
    "    string += '*/\\n'\n",
    "    string += f'columns:{col_list}\\n'\n",
    "    return string\n",
    "\n",
    "def generate_short_answer(table, question):\n",
    "    answer_prompt = f\"\"\"\n",
    "    Here is the table to answer this question. Answer the question in 3-4 words max.\n",
    "    {table}\n",
    "    Question: {question}\n",
    "    The answer is: \n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in answering questions from tabular data.\"},\n",
    "        {\"role\": \"user\", \"content\": answer_prompt}\n",
    "    ]\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        temperature=0,\n",
    "        messages=messages\n",
    "    )\n",
    "    generated_answer = completion.choices[0].message.content.strip()\n",
    "    return generated_answer\n",
    "\n",
    "def evaluate_qa_pair(qa_pair, correct_answers_list):\n",
    "    table_path = rf\"D:\\LLMTables\\Question Generation\\test\\{qa_pair['table_name']}.csv\"\n",
    "    table = convert_to_pipe_format(table_path)\n",
    "    question = qa_pair['question']\n",
    "    generated_answer = generate_short_answer(table, question)\n",
    "    correct_answer = qa_pair[\"result\"]\n",
    "    if generated_answer == correct_answer:\n",
    "        correct_answers_list.append(qa_pair)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"incorrect answer\")\n",
    "        print(question)\n",
    "        print(\"actual: \" + correct_answer)\n",
    "        print(\"generated: \" + generated_answer)\n",
    "        return False\n",
    "    \n",
    "def process_evaluation(json_data):\n",
    "    total_questions = len(json_data)\n",
    "    print(\"total questions: \" + str(total_questions))\n",
    "    correct_answers = 0\n",
    "    incorrect_answers = []\n",
    "\n",
    "    for qa_pair in json_data:\n",
    "        if evaluate_qa_pair(qa_pair, correct_answers_list=[]):\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            incorrect_answers.append(qa_pair)\n",
    "    accuracy = (correct_answers / total_questions) * 100\n",
    "    return correct_answers, accuracy, incorrect_answers\n",
    "\n",
    "def save_incorrect_answers(incorrect_answers, output_path):\n",
    "    with open(output_path, \"w\") as json_file:\n",
    "        json.dump(incorrect_answers, json_file, indent=4)\n",
    "    print(f\"Incorrectly answered questions saved to {output_path}\")\n",
    "\n",
    "def evaluation_pipeline(input_json_path, incorrect_output_json_path):\n",
    "    with open(input_json_path, \"r\") as file:\n",
    "        json_data = json.load(file)\n",
    "    correct_answers, accuracy, incorrect_answers = process_evaluation(json_data)\n",
    "    print(f\"Total Correct Answers: {correct_answers}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    save_incorrect_answers(incorrect_answers, incorrect_output_json_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_json_path = r\"D:\\LLMTables\\Question Generation\\test\\natural_language_output.json\"  \n",
    "    incorrect_output_json_path = r\"D:\\LLMTables\\Question Generation\\test\\incorrect_answers.json\"  \n",
    "    evaluation_pipeline(input_json_path, incorrect_output_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-tables-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
